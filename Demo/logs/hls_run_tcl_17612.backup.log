  **** HLS Build v2025.1 6135595
Sourcing Tcl script 'C:/NCKH/LLama2_110M-Inference_Architecture_on_FPGA/Demo/tb.tcl'
INFO: [HLS 200-1510] Running: open_project llama_layer_prj 
WARNING: [HLS 200-2182] The 'llama_layer_prj' project will not automatically appear within Vitis IDE workspaces and is meant only for TCL batch use.  Please use open_component instead of open_project/open_solution to generate Vitis IDE compatible component files and directory structure.
Resolution: For help on HLS 200-2182 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2025.1%20English&url=ug1448-hls-guidance&resourceid=200-2182.html
INFO: [HLS 200-10] Opening solution 'C:/NCKH/LLama2_110M-Inference_Architecture_on_FPGA/Demo/llama_layer_prj'.
INFO: [HLS 200-1510] Running: set_top llama_layer 
INFO: [HLS 200-1510] Running: add_files llama_layer.cpp 
INFO: [HLS 200-10] Adding design file 'llama_layer.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_MHSA.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_MHSA.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_MHSA.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_MHSA.hpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_Rope.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_Rope.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_Rope.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_Rope.hpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_MatMul.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_MatMul.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_MatMul.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_MatMul.hpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_Softmax.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_Softmax.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_Softmax.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_Softmax.hpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_RMS_Norm.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_RMS_Norm.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_RMS_Norm.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_RMS_Norm.hpp' to the project
INFO: [HLS 200-1510] Running: add_files tensor.hpp 
INFO: [HLS 200-10] Adding design file 'tensor.hpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_FFN.cpp 
INFO: [HLS 200-10] Adding design file 'kernel_FFN.cpp' to the project
INFO: [HLS 200-1510] Running: add_files kernel_FFN.hpp 
INFO: [HLS 200-10] Adding design file 'kernel_FFN.hpp' to the project
INFO: [HLS 200-1510] Running: add_files -tb tb.cpp 
INFO: [HLS 200-10] Adding test bench file 'tb.cpp' to the project
INFO: [HLS 200-1510] Running: open_solution solution1 
INFO: [HLS 200-10] Opening solution 'C:/NCKH/LLama2_110M-Inference_Architecture_on_FPGA/Demo/llama_layer_prj/solution1'.
INFO: [SYN 201-201] Setting up clock 'default' with a period of 4ns.
WARNING: [HLS 200-655] The device family 'versalhbm-2MHP' is new to HLS - using 'versalaicore-2HP' characterization library
INFO: [HLS 200-1611] Setting target device to 'xcv80-lsva4737-2MHP-e-S'
INFO: [HLS 200-1505] Using flow_target 'vivado'
Resolution: For help on HLS 200-1505 see docs.xilinx.com/access/sources/dita/topic?Doc_Version=2025.1%20English&url=ug1448-hls-guidance&resourceid=200-1505.html
INFO: [HLS 200-1510] Running: set_part xcv80-lsva4737-2MHP-e-S 
WARNING: [HLS 200-655] The device family 'versalhbm-2MHP' is new to HLS - using 'versalaicore-2HP' characterization library
INFO: [HLS 200-1510] Running: create_clock -period 4 -name default 
INFO: [HLS 200-1510] Running: csim_design 
INFO: [SIM 211-2] *************** CSIM start ***************
INFO: [SIM 211-4] CSIM will launch CLANG as the compiler.
INFO: [HLS 200-2036] Building debug C Simulation binaries
   Compiling ../../../../tb.cpp in debug mode
   Generating csim.exe
--- Bắt đầu Testbench cho Llama Layer (dựa trên run.cpp) ---
DEBUG: config read: dim=768 hidden_dim=2048 n_layers=12 n_heads=12 n_kv_heads=12 vocab_size=32000 seq_len=1024
Đã nạp config và 109529856 trọng số từ C:/NCKH/LLama2_110M-Inference_Architecture_on_FPGA/Demo/stories110M.bin

--- 20 phần tử đầu tiên của các khối trọng số ---
token_embedding_table: -0.0224137 -0.0145511 0.00128661 -0.0178788 -0.0402432 0.0219014 0.0472692 0.0111523 0.0294048 0.0156689 -0.000182251 0.0503941 0.0269191 0.0433961 -0.0339103 0.0308827 0.0214133 -0.0166515 0.024012 -0.0675581 
rms_att_weight: 0.128821 0.115769 0.111907 0.131507 0.122284 0.139558 0.113547 0.147413 0.130267 0.10622 0.128081 0.113858 0.122429 0.116557 0.144837 0.114131 0.134115 0.150637 0.110135 0.672292 
wq[layer 0]: 0.0356931 0.00303994 -0.0696875 0.0044753 -0.00824255 -0.0159624 -0.0196693 0.00163062 -0.0426718 -0.0239346 0.0704324 -0.0103739 -0.0215091 0.0393774 0.0670594 -0.0869166 -0.0198831 -0.0641189 -0.0219682 0.072246 
wk[layer 0]: -0.0211109 -0.00869436 0.0445335 -0.00727346 0.00742399 0.048986 0.0576494 0.00702634 0.0390657 -0.045086 0.0195053 -0.00206642 1.94351e-05 0.0371236 -0.0694777 -0.028845 0.0460222 -0.0106428 0.0319475 -0.0235805 
wv[layer 0]: -0.000324137 0.00125336 0.0748279 -0.0150818 -0.00988501 -0.0212693 0.0123548 0.030465 0.0332843 0.00674863 -0.00534009 0.0196892 -0.039067 0.0124122 -0.0313378 0.0577935 0.0255266 0.00330311 -0.0173033 -0.000374218 
wo[layer 0]: 0.0366974 -0.0531639 -0.00233748 0.07035 -0.013546 0.0187326 0.000503897 0.0134871 0.021402 -0.0140861 -0.047267 -0.0393356 -0.0383328 -0.0311395 0.0423766 -0.0252 -0.0258806 0.0372701 0.0392705 0.0308817 
wq[layer 1]: 0.0165254 0.0149468 0.0278838 -0.0391451 0.0146839 0.0247681 0.0233773 -0.00613782 0.0215114 -0.00308973 -0.0352979 0.0460043 0.0123293 0.0211085 0.0354609 -0.00798506 0.00860235 0.00048701 -0.0300033 0.01836 
wk[layer 1]: -0.0145673 -0.0258295 0.0200554 0.0137536 -0.00881559 0.0114421 0.047017 -0.0105682 -0.0572744 0.0101815 0.0329154 0.0133632 -0.00448253 0.0338058 -0.0185075 0.00942605 0.0214571 -0.01548 -0.0222235 0.0497751 
wv[layer 1]: -0.0190408 0.0268649 0.0184265 -0.0140033 -0.0177252 -0.005912 0.0272315 -0.0185699 0.0215801 0.0497663 0.00908228 -0.0190155 -0.032608 0.0152908 -0.00215851 0.0167425 0.0599532 -0.0518696 -0.0162129 0.00264852 
wo[layer 1]: -0.0209924 -0.0533459 0.00885688 0.0245211 0.00842945 0.0143953 0.0160586 -0.00925546 -0.0485167 0.0183002 -0.0210433 -0.02925 -0.0137355 -0.00732512 -0.0109241 0.0225133 -0.021167 0.0110054 0.00338569 0.00558506 
wq[layer 2]: 0.00120945 -0.0362996 0.0166835 -0.0307475 0.0144081 -0.00479738 0.00955581 -0.010162 -0.0190032 -0.0213455 -0.0353187 0.0499917 0.0295928 0.0390063 -0.0323159 0.0473914 0.000607804 -0.0363853 -0.0413219 0.0103714 
wk[layer 2]: 0.0223183 0.0182836 0.0163737 -0.0217005 0.00152694 0.0218545 -0.00834549 -0.0177471 -0.0417612 0.0444373 0.000243577 0.0249687 -0.00937797 -0.0112629 0.00988361 0.0248477 -0.0250792 -0.0047669 0.0114178 0.0256972 
wv[layer 2]: -0.00898868 -0.0159233 -0.0252518 0.0158699 -0.0056445 -0.00840436 -0.0021556 0.00897678 0.000392908 0.0165058 -0.0180136 0.0262308 0.0156743 0.00548659 -0.00199513 0.00657187 0.00880746 -0.0184507 -0.025616 -0.0113163 
wo[layer 2]: 0.0238273 0.0278257 0.00323969 0.0184033 0.00993206 0.00675555 0.0214928 0.00866337 0.00250308 0.0132274 0.0424341 0.00816625 -0.036256 -0.0077767 -0.0155194 0.0227157 -9.88003e-05 -0.0296827 -0.0157233 0.0609774 
wq[layer 3]: -0.0635386 -0.0756216 0.0483456 -0.198278 0.0305965 -0.0311707 -0.0599207 0.0471661 0.0382022 -0.108371 -0.101667 0.116798 0.156048 0.063992 -0.034542 -0.00534148 -0.0222757 -0.0295494 -0.00703709 0.208526 
wk[layer 3]: -0.00704491 0.0217143 0.0387949 -0.0103168 -0.0248453 0.0568066 -0.042196 -0.0248345 -0.00606814 0.0319075 0.00179933 -0.00715433 -0.0142638 -0.0365464 0.00600923 -0.0588417 -0.0682865 0.00413803 -0.0275436 -0.00799046 
wv[layer 3]: -0.0104023 0.00330573 -0.0141877 -0.0543231 0.0349372 0.00534479 0.00898394 -0.0180783 0.0275893 0.0292759 0.0200847 0.0110871 -0.0178782 -0.0301707 0.0385355 0.0543941 -0.0370642 -0.0201594 -0.0131294 0.00168555 
wo[layer 3]: -0.0368132 0.0107847 0.0359237 -0.0178203 0.0181568 0.00594038 -0.0100924 0.0202657 -0.000886449 0.0419094 0.0228387 -0.043672 -0.0166056 -0.00389936 -0.0124216 -0.012851 0.0235086 0.0491871 -0.010464 -0.0118895 
wq[layer 4]: 0.00231938 0.00458999 -0.00433068 0.0203806 0.00692168 0.00565672 -0.0169496 -0.0253237 0.0108093 0.038477 -0.0145227 0.00389519 -0.0232533 -0.00496856 -0.0481425 0.00228595 0.00959028 0.0684391 0.0409144 -0.0347454 
wk[layer 4]: -0.000763209 -0.0234563 -0.0159696 0.00623476 0.0363571 -0.0145012 -0.0249882 0.0539307 -0.0353586 -0.0313795 -0.0145801 -0.017603 0.00766453 0.0109185 0.000387683 -0.00534437 -0.032827 -0.00160647 -0.00793657 0.0539436 
wv[layer 4]: -0.0202248 -0.00949842 -0.00159548 0.0144748 0.00612431 -0.00253964 0.0116076 -0.00951977 -0.000198217 0.0186288 0.00414637 0.058839 0.0182749 -0.0161994 -0.0125152 -0.00316226 -0.033839 0.0116404 0.00414557 0.00604378 
wo[layer 4]: 0.0133659 0.0145638 -0.0281318 -0.0220729 0.0547496 0.00262343 0.0165768 0.0397984 -0.0190969 0.032254 0.00428432 -0.0190701 -0.0146425 -0.000467883 0.0244706 -0.000137628 -0.0330103 -0.00753303 0.0316049 -0.0316042 
wq[layer 5]: -0.0218749 -0.0140917 0.042323 -0.0275575 0.00637579 -0.0209464 0.0258276 -0.0261688 0.0305106 -0.0598657 -0.00377949 0.00359678 -0.00731381 0.0169089 0.0291595 0.000286992 -0.00570062 0.0199795 0.00202807 0.000792812 
wk[layer 5]: 0.039514 -0.0197457 0.000404052 -0.00995241 0.00975809 -0.0297121 -0.0186175 -0.00875791 -0.0111549 -0.00712798 -0.0332908 0.028524 0.0153539 0.00724324 0.0144927 -0.00401338 0.0127414 0.00701683 0.0205662 0.0199828 
wv[layer 5]: 0.0262112 -0.0144436 0.0264903 0.00736433 0.0152931 0.00261931 0.0186044 -0.0261378 0.00484219 0.0151508 -0.00428045 0.0145925 0.035464 -0.00929687 -0.0226534 -0.0250075 -0.000247176 -0.0433481 -0.0151432 0.0180928 
wo[layer 5]: -0.0420571 0.0159842 0.0161561 -0.00830475 0.0288371 -0.0334391 0.000269602 0.00303454 -0.023892 -0.0225261 0.00274993 0.0164246 0.0356341 -0.0324218 -0.000985194 0.0105901 0.0509874 0.0210231 -0.00253396 0.000291329 
wq[layer 6]: -0.00155824 0.00164006 0.0330516 -0.0481999 0.00539416 -0.0250743 -0.00991386 -0.00389346 0.00382323 -0.0154417 0.00575328 -0.021385 -0.00816494 -0.0170777 -0.0149731 0.00875081 0.00752224 -0.0149593 0.0381851 -0.00422991 
wk[layer 6]: 0.0117649 -0.00114746 -0.0180087 0.00431666 0.0257217 -0.0444514 -0.0124337 -0.0139431 -0.00053398 -0.0213292 -0.0269068 0.0102475 0.0144808 -0.00694545 0.012659 0.0203896 0.0262911 0.00287141 -0.0267511 0.00134836 
wv[layer 6]: -0.00910386 -0.00976297 -0.0186578 0.0146217 0.0350676 0.036359 0.0160741 -0.0104259 -0.0176122 -0.0316474 0.00233337 0.00937181 0.0149414 -0.0251853 -0.021162 -0.0117781 -0.0239604 0.0071494 0.0148632 -0.0155536 
wo[layer 6]: 0.0122133 -0.0403723 -0.0260838 0.0345087 -0.0134636 -0.00958566 0.0153602 -0.00277478 -0.0181425 -0.00151987 -0.0362426 0.0140611 0.0470427 0.010788 0.00782713 -0.00249062 -0.00485345 0.0141677 0.00498521 0.00981419 
wq[layer 7]: 0.0078933 -0.0425438 0.0026952 -0.0327082 0.00448718 -0.00832009 -0.0237686 0.0213986 0.0319157 0.00659349 0.0125755 0.0337589 0.00466563 0.0462621 -0.0208894 -0.0440326 -0.0117057 -0.00933616 0.0160151 -0.00635791 
wk[layer 7]: -0.00526745 -0.00283091 -0.0121725 0.00449458 -0.0234454 0.0274184 0.0156704 -0.0187862 0.0205872 0.0127644 -0.0253527 0.0319117 -0.00234221 0.0325147 0.0219568 0.0172596 0.0345969 -0.00875259 -0.0343038 0.0254184 
wv[layer 7]: 0.0145417 0.0214783 -0.0150518 0.0128459 -0.00333439 0.000867061 -0.00591514 0.0698852 -0.0227078 0.0285286 -0.0146252 -0.0188021 0.00103012 -0.0305182 -0.0135459 0.00472849 -0.0158581 0.0112238 0.0298166 0.00184557 
wo[layer 7]: -0.00271415 0.0223929 0.0186064 0.025346 -0.036278 0.0173104 -0.000311204 -0.038947 -0.0386319 -0.00605978 0.00931668 0.0137038 -0.00683962 -0.0260098 -0.01764 0.0147601 -0.032767 0.0379088 0.0313256 -0.014995 
wq[layer 8]: -0.0143706 -0.0170611 -0.00761077 0.00197703 -0.0107953 0.00771548 -0.0103198 -0.00358628 0.0604065 0.0287878 0.00745431 -0.0234413 0.00598983 -0.0276343 -0.00410744 -0.021212 -0.00843459 -0.00332123 0.00522922 0.0562783 
wk[layer 8]: -0.000745545 -0.00378085 0.0128465 0.0357174 -0.0177388 0.0134434 0.0612519 -0.0434426 -0.00109913 0.00891131 0.00482818 0.0432389 -0.0197684 0.00959467 -0.0307868 -0.0111969 0.00528172 0.0369414 0.0476111 0.00501163 
wv[layer 8]: -0.0438611 0.0203819 0.0180346 0.0133962 -0.0268507 0.11182 0.00238829 -0.0586923 -0.00365347 -0.00213473 -0.000921627 0.0260205 0.0356486 -0.0272113 -0.0116256 -0.0394567 0.0419191 0.0364249 -0.00503859 0.0406616 
wo[layer 8]: -0.0281659 -0.0107215 -0.00253152 -0.0243425 -0.01946 -0.0358909 -0.0194207 0.00371033 0.0460268 0.0428048 -0.00176552 -0.00871734 0.0408812 -0.0147445 -0.00262366 0.00381848 -0.0606559 0.0378908 -0.0167781 0.0214905 
wq[layer 9]: 0.0232344 -0.00031291 0.0110371 0.0445528 -0.00817028 -0.00717304 -0.0118613 -0.00311054 0.00938808 0.0104929 -0.0261702 0.02148 -0.00877024 0.00269397 0.0356464 0.00356174 0.000948947 -0.00188517 -0.0100858 -0.00224064 
wk[layer 9]: 0.0280041 -0.0410084 -0.000999128 -0.0151914 0.02032 -0.0236345 0.0245197 0.0100659 -0.0177569 0.0110156 -0.000601807 -0.0389788 0.00967188 -0.00290792 -0.00490827 -0.042507 -0.00674734 0.0109391 -0.0296956 -0.0293685 
wv[layer 9]: 0.0204596 -0.0320066 -0.0241847 -0.0374561 -0.00767556 0.00155542 -0.00865302 0.0170499 -0.01587 -0.0132189 -0.00748505 0.0172662 0.0118809 -0.00873057 0.00264846 -0.0300724 -0.0206443 0.0232124 -0.00809705 0.0220823 
wo[layer 9]: -0.0090858 -0.0124603 -0.0155976 -0.00403628 0.0118685 0.0175209 0.00851731 -0.00987169 0.00761422 -0.00351717 0.018996 0.00430377 -0.00870838 0.00132506 0.00619755 -0.00630797 -0.0168437 0.0268864 -0.0405655 0.0070454 
wq[layer 10]: -0.0184356 0.00441112 0.00172248 0.0119165 0.0124838 0.00394438 0.0235971 -0.0178353 0.0171928 -0.0103004 -0.0163482 0.011369 -0.0388862 -0.0102128 0.043939 0.0289818 0.000292807 0.0423076 0.00664986 0.000280648 
wk[layer 10]: 0.0203781 0.0241683 0.00808321 0.00428357 -0.0213158 -0.00503593 0.0150237 -0.0101683 -0.0203434 -0.0108887 -0.021366 0.00611272 0.0213439 -0.000359457 0.0119804 0.00659253 -0.0131106 0.00233537 0.00274941 -0.0577933 
wv[layer 10]: 0.0252714 -0.00931108 0.0377793 -0.0773068 0.008061 0.00123917 -0.0180368 -0.0246656 -0.0182302 0.0250894 0.0167349 0.00142891 0.00297786 -0.021338 0.00137182 0.0472077 0.0178562 0.0275179 0.0319526 -0.0128952 
wo[layer 10]: -0.0480199 -0.00394764 0.0179116 0.000193261 0.0216509 -0.0320892 -0.0650995 -0.0815288 -0.00604526 -0.00213825 -0.0368971 0.0546384 -0.000757625 -0.000255609 -0.0342103 -0.0468655 0.0103948 0.00121232 0.0165941 -0.00244362 
wq[layer 11]: -0.0232629 0.0180702 -0.0216138 0.0314252 -0.0134119 0.011764 0.0198701 0.0264117 0.00960581 -0.0193478 -0.0196759 -0.0332037 -0.0180202 0.00441692 0.00657043 0.00364253 -0.00867013 -0.00175932 0.0265022 0.0681012 
wk[layer 11]: 0.0227882 -0.0208079 -0.0316373 -0.0158468 -0.00398646 0.0367836 0.0209063 0.038062 -0.00750317 -0.0237373 0.00321122 0.0381774 0.016935 -0.0187665 -0.0302792 0.034933 0.00937419 -0.0207657 0.0375423 0.0162593 
wv[layer 11]: -0.0398387 0.00807108 -0.0106036 0.0155557 -0.0313783 -0.0492326 0.0348663 -0.0189372 0.0199041 -0.026631 0.0155803 -0.0379732 -0.0334889 0.00644066 -0.00494676 -0.0510597 0.0205915 0.00635586 -0.0253147 0.0279828 
wo[layer 11]: -0.0269081 -0.0158158 -0.0146674 0.00174177 0.0254632 -0.0634601 0.0116321 0.0275986 -0.0338752 0.0292353 -0.0286409 0.0148684 0.02152 -0.0190582 -0.0259709 -0.00572775 -0.00639144 0.0252609 -0.0238035 0.0192088 
rms_ffn_weight: 0.424805 0.441208 0.43294 0.424511 0.368091 0.374236 0.403975 0.426325 0.416315 0.402175 0.418473 0.42489 0.432342 0.397847 0.432209 0.418922 0.419072 0.418864 0.421679 0.0657692 
w1[layer 0]: 0.00216184 -0.0212844 -0.0397819 -0.0289602 -0.00594303 -0.0407787 -0.0124105 -0.00251742 0.012819 -0.0226389 0.00338979 -0.00296496 0.00323948 -0.00101027 0.000642927 -0.00598732 0.0358745 0.0292852 0.0122512 0.0273571 
w2[layer 0]: -0.0522058 -0.034372 -0.0100975 -0.00484145 0.0205462 0.0024856 -0.00224314 0.0464335 0.023337 -0.0359599 0.00928246 -0.00527738 -0.00531806 0.00925106 -0.000310121 0.0436939 0.0188677 -0.0709157 -0.0583083 -0.0101828 
w3[layer 0]: -0.0298947 0.0255094 0.0354433 0.00754613 -0.0232288 0.00794773 -0.0021837 0.0372634 0.0138628 0.011496 -0.0247071 0.00639186 -0.0182662 0.0210444 -0.0183181 -0.00811568 -0.00355129 -0.0206704 -0.0428165 -0.0410331 
w1[layer 1]: -0.000202547 -0.0140911 -0.0236233 -0.0175023 0.0342146 0.0187278 0.0139294 0.00448198 0.0107274 -0.00112575 -0.0141917 -0.00984656 -0.0440935 -0.0126913 -0.0517447 -0.0205643 -0.0320713 -0.014129 -0.0191588 -0.0220859 
w2[layer 1]: -0.0238316 -0.00970999 -0.0305157 -0.0573946 0.0153375 0.0229365 -0.0068377 0.0508533 -0.0247388 0.0123209 0.0133225 0.0209824 -0.032586 0.0747145 -0.0269139 0.0262383 0.0168681 0.00480199 0.0133978 -0.0338774 
w3[layer 1]: 0.0287326 0.0374976 -0.0112378 -0.0185396 0.0364257 -0.0131208 0.000391048 0.0360862 -0.00549164 -0.027569 0.0248681 -0.0570791 -0.018445 -0.00251657 -0.0432265 0.0217263 -0.0162159 0.0159229 -0.0137667 0.0408021 
w1[layer 2]: -0.0442083 0.0370126 -0.0422503 0.00686673 0.0545282 -0.0292734 0.000208097 0.0216587 -0.06895 -0.0238508 0.0432393 0.0537082 -0.0196638 -0.0427839 0.00353581 -0.0225408 -0.00221528 -0.00117021 0.0090847 -0.0433225 
w2[layer 2]: 0.0372843 0.0243099 -0.0225088 0.00424332 0.0177794 0.035139 -0.0361384 -0.0176897 -0.0384206 -0.00515406 -0.0439834 -0.0178385 8.06723e-05 0.0453993 0.00965334 -0.0569008 0.0131778 -0.0283775 0.0231157 0.028047 
w3[layer 2]: 0.0518043 0.0574299 0.0329932 -0.0157753 -0.0324259 0.0179103 0.000285489 0.0010559 0.0253409 0.00446582 -0.070444 -0.0462693 0.0269716 0.0183112 -0.00849091 -0.0287926 0.0103266 0.0406993 0.0129331 0.018887 
w1[layer 3]: -0.00525405 0.0359763 -0.00613856 0.0238534 -0.022607 0.030161 0.0248117 -0.000591079 0.0273231 -0.00320228 -0.00989667 0.0214408 -0.0210549 0.0701736 0.00292396 -0.0141585 0.00120011 -0.00108265 0.00749845 0.00217154 
w2[layer 3]: -0.00171527 0.0183816 -0.0403313 -0.0170892 0.000696355 0.0011999 -0.0330333 0.015985 -0.0172567 0.0239012 0.0266514 -0.020937 -0.0164287 -0.0142342 -0.0395616 -0.0402448 0.0138335 -0.00703623 -0.0109724 -0.0386688 
w3[layer 3]: 0.0349327 -0.0647426 0.00564567 0.023182 0.00780351 0.000992981 -0.0142736 0.0109277 0.0356046 -0.0289513 -0.00238416 0.0358012 0.0253446 0.0130624 0.0169196 0.0687255 0.0284252 -0.0795643 0.0412654 -0.0224411 
w1[layer 4]: -0.00400111 0.00363379 0.0261136 0.00317688 0.00760307 -0.0107867 0.0149517 0.00726857 0.0105013 -0.00233746 -0.00599472 -0.00697028 -0.0183679 0.0355651 -0.0159803 0.039671 -0.0401517 0.00656451 -0.0223808 -0.00275838 
w2[layer 4]: 0.0175769 0.00712073 0.0114871 -0.0161417 0.0551986 0.027013 0.0358864 -0.0308018 -0.0261912 0.0108951 0.011793 -0.000912554 -0.0232091 -0.0171584 0.0150802 0.0037545 -0.0264308 -0.0161961 -0.00489595 0.0134701 
w3[layer 4]: 0.00653978 -0.0357569 0.0410446 0.0918819 -0.0327411 0.0222337 0.0224928 -0.0153218 -0.0305805 0.0174537 0.0437942 -0.00902124 -0.00972252 0.022938 0.0135674 0.02044 -0.0340368 -0.0055218 0.00480654 -0.0319802 
w1[layer 5]: 0.00978268 -0.0108249 -0.00273197 -0.000305167 0.0928544 0.0173448 0.0169735 0.0268622 -0.00317416 0.0141643 0.00800463 -0.0542809 -0.00462179 0.00392992 -0.00874458 0.0190363 0.00113033 -0.0646058 -0.00434304 -0.0565447 
w2[layer 5]: -0.00550199 0.011026 -0.0522184 0.00383332 -0.00212586 0.0148387 0.032328 0.0273014 0.0183533 -0.00669684 0.0421058 -0.0459059 -0.00259605 0.030507 -0.0465469 -0.00403274 0.0371094 -0.0110512 0.019534 0.0164943 
w3[layer 5]: 0.0356734 0.0353618 0.0405337 0.0272116 -0.0236749 0.0239686 0.0411656 -0.0171232 -0.0263908 -0.00980045 -0.0039309 0.0527718 0.0594312 -0.000252367 -0.0266519 -0.0163625 0.0167596 -0.00503726 0.0398882 -0.00467887 
w1[layer 6]: 0.0259632 0.00452317 0.0109649 -0.0170664 -0.0194916 -0.00766538 -0.00373822 0.00672735 0.0115641 -0.0167837 -0.0203973 0.00399284 -0.0209775 0.0149246 -0.00836544 0.0524336 -0.0347726 -0.00730439 -0.00912261 0.0481667 
w2[layer 6]: 0.0393466 0.0278461 -0.0151952 -0.00458715 0.0121791 -0.012334 0.037918 0.0270245 -0.0278396 -0.00792766 0.0234869 0.0742004 -0.00844528 -0.00511254 -0.0271017 -0.0119179 0.00267518 0.00272232 -0.0256853 -0.0346614 
w3[layer 6]: -0.0447977 -0.024682 -0.0443924 0.0164931 0.000666829 0.0238569 -0.00492059 -0.0125791 0.0106897 -0.0324142 0.037169 0.0148337 0.00757473 -0.00165273 4.56051e-05 -0.0445505 -0.00510208 -0.0190439 -0.0119116 0.00548411 
w1[layer 7]: -0.0537822 0.0158031 0.0386476 0.00660099 0.0102644 -0.0193175 0.0220486 0.0342105 0.0200276 -0.00532583 -0.00433369 -0.0278349 -0.0641487 -0.022091 -0.00959777 -0.0115105 0.0288195 -0.0152485 -0.0447559 -0.012549 
w2[layer 7]: 0.0102021 0.0150985 -0.0153896 -0.0544041 -0.00762797 -0.016654 -0.0356037 -0.0119057 0.0163666 -0.0201162 -0.0224923 -0.0151082 0.0257158 -0.031851 0.0771537 -0.000553419 -0.0349446 0.00310384 -0.0208031 0.0145786 
w3[layer 7]: -0.0314399 -0.0342267 -0.010129 -0.0347799 -0.0234669 0.023326 0.0247844 -0.0469643 0.0193488 0.0180487 -0.00729722 -0.0130087 -0.0364442 -0.0171608 0.0130832 -0.0459188 0.0273125 0.00872897 0.0336727 0.000163801 
w1[layer 8]: 0.0156131 0.0478352 0.0304821 0.0106806 0.0125121 0.00940101 -0.0112127 -0.0185547 -0.00993553 0.0501965 0.0158227 -0.0136803 -0.0329628 -0.0189079 -0.023498 -0.00465644 -0.00517141 0.0055936 -0.0216992 0.0622929 
w2[layer 8]: 0.000796579 0.0132602 0.0331128 0.0157104 -0.052101 0.00932916 -0.0230002 -0.0226342 -0.0314645 0.0677891 0.0109255 0.0131275 -0.00920673 0.00954752 0.0136302 -0.0291455 -0.016934 0.0184251 0.0503313 -0.0310662 
w3[layer 8]: -0.00939202 0.0300744 0.0151076 0.0347152 -0.0200907 0.0644057 0.00395847 0.0193424 -0.0110464 0.0409596 -0.0300788 -0.016998 0.0417429 -0.0181416 -0.0474371 0.0178418 -0.000100425 -0.0450155 -0.0136898 -0.0056952 
w1[layer 9]: 0.0251886 -0.00237068 0.0342434 -0.010877 -0.00832785 0.00670333 0.0155584 -0.0101886 0.0146631 -0.0310438 0.00954616 0.0503251 -0.0065528 0.00950914 0.00548432 0.0586849 0.0305668 0.0290859 -0.0129645 0.0300303 
w2[layer 9]: 0.0419472 0.00430338 -0.0117811 0.0581054 0.0164073 -0.0140563 -0.041452 -0.015857 0.00579809 -0.017263 -0.0146844 -0.0386306 -0.0291602 -0.0236452 -0.00291824 -0.0135419 0.00629713 -0.00896501 0.0122591 -0.0384431 
w3[layer 9]: -0.00116984 0.0189235 -0.0274926 0.0165493 0.0311525 -0.00998887 0.0108146 -0.025266 0.00825643 -0.0315952 -0.0150873 -0.0269495 -0.0182243 0.00622757 0.0117206 0.00941871 -0.00148257 -0.0227758 0.00167576 -0.0412927 
w1[layer 10]: 0.0157911 0.0272333 0.0262179 -0.0247705 0.0086792 0.0108003 0.0308789 0.00676596 0.0212304 -0.00921815 -0.0161246 0.0405358 0.0203504 -0.0158868 0.0453721 -0.0103042 -0.00221174 0.0304174 -0.0130256 0.0244895 
w2[layer 10]: -0.0124825 -0.0135606 0.0170095 -0.0154432 -0.00752589 -0.0352328 -0.0119121 -0.0103572 -0.0333398 -0.0158623 0.0297271 0.0272417 -0.0147038 -0.00754672 0.0104484 0.024325 0.0370264 0.0166123 -0.0327593 -0.0263651 
w3[layer 10]: -0.00742105 0.00372472 -0.0123802 -0.0506624 -0.035777 -0.0311109 -0.0444687 0.0378823 0.0189007 0.0130195 0.0100525 -0.0121619 -0.0185872 -0.0150399 -0.0294178 -0.0088818 0.0073822 -0.0260228 -0.00902912 0.0265483 
w1[layer 11]: 0.0354141 -0.0116983 0.0296525 -0.00972544 -0.058111 0.006976 0.0159353 0.0686688 -0.0354044 -0.00567623 0.010254 0.0510487 0.00561672 -0.00812314 -0.0355155 -0.0429332 -0.0117269 -0.0455241 0.0296053 -0.0117453 
w2[layer 11]: -0.00612807 0.0341647 0.0158737 0.000103739 0.0369527 -0.00978974 -0.0365189 0.0180876 -0.0269131 -0.0273059 -0.0505403 0.000227428 -0.0171095 -0.0403434 -0.0476966 0.00845248 -0.0230832 0.0379696 -0.0208445 -0.0126141 
w3[layer 11]: 0.0259466 -0.0429828 9.17628e-05 -0.00233406 -0.00525321 0.0029974 -0.00451295 -0.0296892 0.0282633 -0.0182885 0.0306401 0.00471917 -0.0412584 0.0335354 0.00823673 0.0591205 0.02267 -0.0243557 -0.0007599 -0.0346442 
rms_final_weight: 3.18261 3.28215 3.05725 3.17514 2.97255 2.85815 3.16689 3.19182 3.1992 3.06711 3.26018 3.29082 3.92144 3.23733 3.52562 3.11766 3.24463 3.13113 3.10441 9.55025 
Thực thi llama_layer...
Thực thi hoàn tất.
10 logits đầu tiên:
  logits[0] = 0.00330278
  logits[1] = -0.00630607
  logits[2] = 0.0194598
  logits[3] = 0.0201487
  logits[4] = 0.0226777
  logits[5] = 0.00839904
  logits[6] = -0.0165651
  logits[7] = 0.00142725
  logits[8] = -0.0286713
  logits[9] = 0.0043771
10 logits cuối cùng:
  logits[31990] = 0
  logits[31991] = 0
  logits[31992] = 0
  logits[31993] = 0
  logits[31994] = 0
  logits[31995] = 0
  logits[31996] = 0
  logits[31997] = 0
  logits[31998] = 0
  logits[31999] = 0
--- Testbench hoàn tất! ---
INFO [HLS SIM]: The maximum depth reached by any hls::stream() instance in the design is 1572864
INFO: [SIM 211-1] CSim done with 0 errors.
INFO: [SIM 211-3] *************** CSIM finish ***************
INFO: [HLS 200-2161] Finished Command csim_design Elapsed time: 00:00:32; Allocated memory: 0.301 MB.
INFO: [HLS 200-112] Total CPU user time: 3 seconds. Total CPU system time: 2 seconds. Total elapsed time: 36.064 seconds; peak allocated memory: 584.195 MB.
INFO: [vitis-run 60-791] Total elapsed time: 0h 0m 37s
INFO: [vitis-run 60-1662] Stopping dispatch session having empty uuid.
